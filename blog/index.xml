<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Nathan Schucher</title>
    <link>https://nathanschucher.com/blog/index.xml</link>
    <description>Recent content in Blog on Nathan Schucher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 25 Jan 2017 15:30:59 -0500</lastBuildDate>
    <atom:link href="https://nathanschucher.com/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>probability theory as an extension of Aristotelian logic</title>
      <link>https://nathanschucher.com/blog/probability-theory-as-an-extension-of-aristotelian-logic/</link>
      <pubDate>Wed, 25 Jan 2017 15:30:59 -0500</pubDate>
      
      <guid>https://nathanschucher.com/blog/probability-theory-as-an-extension-of-aristotelian-logic/</guid>
      <description>

&lt;p&gt;Probability has always been a bit of a mystery to me. Manipulating basic probabilities according to the axioms of probability is fine. Venn diagrams representing joint distributions is fine. Reasoning about continuous probability is&amp;hellip; fine. The calculations and algebra all makes sense, but I never got it. Probability Theory: The Logic of Science by Edward Jaynes starts from the ground up in a way that makes sense to me. Not only because it has so far avoided measure theory and infinite sets, but also because the goal is to design a robot that can reason (I think that&amp;rsquo;s pretty rad).&lt;/p&gt;

&lt;h1 id=&#34;aristotelian-logic&#34;&gt;Aristotelian Logic&lt;/h1&gt;

&lt;p&gt;Jaynes argues for development of a system of probability that acts as an extension of Aristotelian logic rather than infinite set theory.&lt;/p&gt;

&lt;p&gt;Aristotelian Logic consists of propositions \(\{A, B, C, .. \}\) and premises ‘if A then B’ which can be acted upon by two arguments:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Major Premise: If A is true then B is true&lt;/p&gt;

&lt;p&gt;Minor Premise: A is true&lt;/p&gt;

&lt;p&gt;Conclusion: B is true&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Or&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Major Premise: If A is true then B is true&lt;/p&gt;

&lt;p&gt;Minor Premise: B is false&lt;/p&gt;

&lt;p&gt;Conclusion: A is false&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, how do we quantify the argument&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Major Premise: If A is true then B is true&lt;/p&gt;

&lt;p&gt;Minor Premise: B is true&lt;/p&gt;

&lt;p&gt;Conclusion: B is more plausible&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;How much more plausible is B, not that we know A? Real numbers can be used to quantify this, so we need a robot that can reason about propositions (A, B, C, …), and their relative plausibilities:&lt;/p&gt;

&lt;h1 id=&#34;degree-of-plausibility-are-represented-by-real-numbers&#34;&gt;Degree of plausibility are represented by real numbers&lt;/h1&gt;

&lt;p&gt;We would to assign a real number representing the plausibility that a proposition A is true given the knowledge that B is true. This is denoted&lt;/p&gt;

&lt;p&gt;$$ A | B $$&lt;/p&gt;

&lt;p&gt;And is pronounced ‘A given B’. This is a real number! It is not a probability, it is not in \([0, 1]\): in fact we know nothing about it other than that it is a real number.&lt;/p&gt;

&lt;h1 id=&#34;qualitative-correspondence-with-common-sense&#34;&gt;Qualitative correspondence with common sense&lt;/h1&gt;

&lt;p&gt;We then &lt;em&gt;choose&lt;/em&gt; to view larger real numbers as representing a higher degree of plausibility. This is an intuitive and natural choice, but is not necessary. We also would like consistency with the rules of Boolean logic: eg. A+B|C represents the plausibility that at least one of A or B is true given C.&lt;/p&gt;

&lt;p&gt;As for correspondence with common sense, for example: if C’ represents new information built upon C, then&lt;/p&gt;

&lt;p&gt;$$(A|C&amp;#39;) &amp;gt; (A|C)$$
we expect
$$(AB|C&amp;#39;) &amp;gt; (AB|C)$$
so long as
$$(B|C&amp;#39;) = (B|C)$$&lt;/p&gt;

&lt;h2 id=&#34;consistency&#34;&gt;Consistency&lt;/h2&gt;

&lt;p&gt;Jaynes posits the following consistency axioms for the robot:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If a conclusion can be reasoned out in more than one way, then every possible way must lead to the same result.&lt;/p&gt;

&lt;p&gt;The robot always takes into account all of the evidence it has relevant to a question. It does not arbitrarily ignore some of the information, basing its conclusions only on what remains. In other words, the robot is completely non-ideological.&lt;/p&gt;

&lt;p&gt;The robot always represents equivalent states of knowledge by equivalent plausibility assignments. That is, if in two problems the robot’s state of knowledge is the same (except perhaps for the labeling of the propositions), then it must assign the same plausibilities in both.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And that’s all. The previous desiderata about common sense, and real numbers, along with these consistency guarantees are enough to develop a mathematical theory of plausible reasoning (ie. probability theory). Notice there is no mention of the familiar axioms of probability theory. Although this is a different formulation, Jaynes insists that he (and theory) agree with Kolmogorov&amp;rsquo;s results without any of the messiness of measure theory and infinite paradoxy.&lt;/p&gt;

&lt;h1 id=&#34;what-about-p-x&#34;&gt;What about \(P(x)\)?&lt;/h1&gt;

&lt;p&gt;Through some relatively straightforward derivation (and references to exhaustive proofs) Jaynes develops the rules of probability that are familiar. For example, we might like to know what (AB|C) is, and the solution involves finding a solution to the equation&lt;/p&gt;

&lt;p&gt;$$ (AB|C) = F[(B|C), (A|BC)] $$&lt;/p&gt;

&lt;p&gt;This is all in the first two chapters of the book. The rest goes onto develop familiar distributions and techniques using this robot I’m still working through it, but the basic premise is compelling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>alternating neural attention for machine reading</title>
      <link>https://nathanschucher.com/blog/alternating-neural-attention-for-machine-reading/</link>
      <pubDate>Wed, 11 Jan 2017 16:31:57 -0500</pubDate>
      
      <guid>https://nathanschucher.com/blog/alternating-neural-attention-for-machine-reading/</guid>
      <description>

&lt;p&gt;&lt;em&gt;March 15th 2036&lt;/em&gt;: You sit with your child of four years, reading Rudyard Kipling’s The Jungle Book. After reading precisely 20 sentences of the story the child turns its face to you and says:&lt;/p&gt;

&lt;p&gt;“Please caregiver, tell me what X represents in this sentence, ‘Yes,’ said X, ‘all the jungle fear Bagheera&amp;ndash;all except Mowgli.’”&lt;/p&gt;

&lt;p&gt;The child has carefully worded the query in this manner because you are not you, you are a Childcare Robot, and this is the only type of query you encountered in your training set. You respond, “Mowgli,” and the child nods knowingly and smiles. You twist your robo-face into your best imitation of a human smile.&lt;/p&gt;

&lt;h2 id=&#34;story-mode&#34;&gt;Story Mode&lt;/h2&gt;

&lt;p&gt;I dumped all the attention outputs from the test set to see what the network was doing. You can pick a story from the drop-down on the left, and drag the slider to see the distributions at each step of the iteration. The labeled answer is listed below as well as the predicted answer from each time step.&lt;/p&gt;

&lt;div class=&#34;story&#34; id=&#34;story-0&#34;&gt;
&lt;div&gt;
&lt;select&gt;&lt;/select&gt;
&lt;label class=&#34;glimpse&#34;&gt;Glimpse&lt;/label&gt;
&lt;input type=&#34;range&#34; min=&#34;1&#34; max=&#34;8&#34; value=&#34;1&#34; step=&#34;1&#34; /&gt;
&lt;label&gt;Answer: &lt;span class=&#34;answer&#34;&gt;&lt;/span&gt;&lt;/label&gt;
&lt;label&gt;Predicted: &lt;span class=&#34;predicted&#34;&gt;&lt;/span&gt;&lt;/label&gt;
&lt;/div&gt;
&lt;label&gt;Document: &lt;/label&gt;
&lt;div class=&#34;document&#34;&gt;&lt;/div&gt;
&lt;label&gt;Query: &lt;/label&gt;
&lt;div class=&#34;query&#34;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;iterative-alternating-neural-attention-for-machine-reading&#34;&gt;Iterative Alternating Neural Attention for Machine Reading&lt;/h2&gt;

&lt;p&gt;I decided to get my hands dirty and implement one of the papers I had been reading. I picked a &lt;a href=&#34;https://arxiv.org/abs/1606.02245&#34;&gt;paper&lt;/a&gt; written by researchers at &lt;a href=&#34;https://mila.umontreal.ca/&#34;&gt;UdeM&lt;/a&gt; and a startup in Montreal called Maluuba (&lt;a href=&#34;http://www.maluuba.com/blog/2017/1/13/maluuba-microsoft&#34;&gt;recently acquired by microsoft&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Machine readers are models that attempt to develop some kind of document understanding in order to apply them to various tasks: summarization, question answering, captioning, sentiment analysis, etc.
There are a &lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;number&lt;/a&gt; of &lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;great&lt;/a&gt; &lt;a href=&#34;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&#34;&gt;articles&lt;/a&gt; written about &lt;a href=&#34;http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html&#34;&gt;recurrent neural networks&lt;/a&gt;, so I won&amp;rsquo;t explain how they work here.&lt;/p&gt;

&lt;h2 id=&#34;the-model&#34;&gt;The Model&lt;/h2&gt;

&lt;p&gt;The model takes as input a document, and a &amp;lsquo;cloze-style&amp;rsquo; query which is a sentence with a word missing. The output of the model is a prediction to fill the blank in the query.&lt;/p&gt;

&lt;h3 id=&#34;bidirectional-gru-for-encoding-the-document-and-query&#34;&gt;Bidirectional GRU for encoding the document and query&lt;/h3&gt;

&lt;p&gt;The document and query are split into tokens, given a unique id, and then used to train word embeddings.
A common use of RNNs in NLP is for collapsing a sequence into a single vector.
We hope that this vector encodes the meaning of the sentence in some high-dimensional space such that we can do interesting things with it!
Many papers, including this one, use bidirectional RNNs in order to capture both the left-to-right semantics and the right-to-left. In this case the outputs of the forward and backward pass are concatenated.&lt;/p&gt;

&lt;h3 id=&#34;attentive-query-glimpses&#34;&gt;Attentive query glimpses&lt;/h3&gt;

&lt;p&gt;The encoded query vectors are used to generate a &lt;em&gt;glimpse&lt;/em&gt; of the query as a whole.
The trick with this paper, is to build two attention distributions, one over the document, and one over the query.
The document attention and query attention distributions represent the likelhood of each token being useful to predicting the answer.
An additional novel feature is to iteratively reconstruct query and document glimpses rather than performing a single pass.
The network alternates between glimpsing at the query, and using the information gleaned to create a document glimpse.
The idea being that many inference problems require more than one logical hop. Then the document glimpse at time t is defined,&lt;/p&gt;

&lt;p&gt;$$\mathbf{d}_t = \sum_{i} d_{i,t} \mathbf{\tilde{d}}_{t}$$&lt;/p&gt;

&lt;p&gt;This acts like an approximation of the expected value of this document attention distribution.
Although it doesn&amp;rsquo;t output samples from the distribution, it produces a new vector positioned near the mode of the attentions.
We hope that this lets the network exploit the linear structure of the embedding space and enable these glimpse vectors to capture the most important properties of the query/document.&lt;/p&gt;

&lt;p&gt;In order for the network to parameterize and control these glimpses, the network maintains a &lt;em&gt;context&lt;/em&gt; vector.
This is the same as the thought vectors mentioned above, and is implemented again as a GRU.
This controls the query and document glimpses over the iterations.
Finally, the last document attention distribution is used to compute the prediction.
By summing over each word (words appear multiple times) we get the total likelihood of that word being the answer, and the highest probability is the network&amp;rsquo;s prediction.&lt;/p&gt;

&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;

&lt;p&gt;Below is a picture from TensorBoard of the likelihood that the network has assigned to the correct answer. The x-axis is the probability [0,1], the y-axis is the density (ie. over the number of examples in that batch), and the z-axis is the training step.&lt;/p&gt;

&lt;p&gt;You can see as the training proceeds (from back to front), the likelihood distribution for the answer shifts from near-zero to near one, demonstrating the improvement over time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nathanschucher.com/img/answer_probability.png&#34; alt=&#34;Likelihood of Answer vs. Time&#34; /&gt;&lt;/p&gt;

&lt;p&gt;My implementation acheived 65% accuracy on the Children&amp;rsquo;s Book Test (Named Entity) test set, compared to the paper&amp;rsquo;s 68.6%.
This was after ~8 epochs, after which the validation loss started to increase.
There are a few things missing from my implementation, including proper orthogonal initialization of the GRU weights, &lt;a href=&#34;http://blog.dennybritz.com/2017/01/17/engineering-is-the-bottleneck-in-deep-learning-research/&#34;&gt;but undoubtedly much more&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you play around with the &amp;ldquo;Story Mode&amp;rdquo; section above, you may notice that the distribution doesn&amp;rsquo;t change &lt;em&gt;that&lt;/em&gt; much, especially the query distribution.&lt;/p&gt;

&lt;p&gt;I was anticipating a more dramatic change in distribution as the network alternates glimpses, but it seems to immediately latch on to names and proper nouns. My initial guess is that the network memorizes those entities because they&amp;rsquo;ve read parts of the same stories in the training set. This is a rather large problem, and so these results can&amp;rsquo;t really demonstrate generalization.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;

&lt;p&gt;I want to apply this model to the other datasets in the original paper: Facebook&amp;rsquo;s bAbI tasks, DeepMinds CNN news dataset along with Maluuba&amp;rsquo;s additional question/answer data. I&amp;rsquo;m also interested in trying to apply &lt;a href=&#34;https://arxiv.org/abs/1603.08983&#34;&gt;Adaptive Computation Time&lt;/a&gt;, which allows an RNN to decide when to halt, rather than using a fixed number of iterations. It seems obvious to me that certain examples will be more difficult than others. Giving the network the ability to halt would allow it to take more computation time on more difficult examples, and maybe give more insight into how the glimpse mechanism works.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>